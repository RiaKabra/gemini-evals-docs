{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c79d78c5-7ea1-4635-b7a5-1d2c063a7e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temperature: 0.1, Top_p: 1.0\n",
      "Response: Why did the computer go to therapy?\n",
      "\n",
      "Because it had too many bytes of emotional baggage!\n",
      "\n",
      "\n",
      "Temperature: 0.5, Top_p: 1.0\n",
      "Response: Why did the computer go to the doctor?\n",
      "\n",
      "Because it had a virus!\n",
      "\n",
      "\n",
      "Temperature: 0.9, Top_p: 1.0\n",
      "Response: Why did the computer keep sneezing?\n",
      "\n",
      "Because it had a virus!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyAMldqf3thYz70VjXrc_L7mcQGPfJV20ug\")\n",
    "\n",
    "\n",
    "def run_with_config(temp, top_p):\n",
    "    response = model.generate_content(\n",
    "        \"Tell a joke about computers.\",\n",
    "        generation_config={\n",
    "            \"temperature\": temp,\n",
    "            \"top_p\": top_p,\n",
    "            \"max_output_tokens\": 100\n",
    "        }\n",
    "    )\n",
    "    print(f\"\\nTemperature: {temp}, Top_p: {top_p}\")\n",
    "    print(\"Response:\", response.text)\n",
    "\n",
    "run_with_config(0.1, 1.0)\n",
    "run_with_config(0.5, 1.0)\n",
    "run_with_config(0.9, 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48c13d9b-bfa0-42cf-8518-aeefba25675a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's what I see in the image:\n",
      "\n",
      "*   **A two-tiered birthday cake:** It's decorated with frosting, sprinkles, hearts, and what appear to be cherries.\n",
      "*   **Candles:** There are numerous lit candles on the top tier of the cake.\n",
      "*   **Cartoon style:** The image has a cartoonish or illustrated style.\n",
      "*   **Celebration/Birthday:** The image suggests a celebration, most likely a birthday.\n"
     ]
    }
   ],
   "source": [
    "image_path = r\"C:\\Users\\riaka\\OneDrive\\Pictures\\Screenshots 1\\Screenshot 2023-03-12 123638.png\"\n",
    "\n",
    "with open(image_path, \"rb\") as img_file:\n",
    "    image_data = img_file.read()\n",
    "\n",
    "# ✅ Step 4: Send image + prompt\n",
    "response = model.generate_content(\n",
    "    contents=[\n",
    "        {\"text\": \"What do you see in this image?\"},\n",
    "        {\"inline_data\": {\n",
    "            \"mime_type\": \"image/png\",\n",
    "            \"data\": image_data\n",
    "        }}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ✅ Step 5: Print result\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5f1c43d-8d16-4558-a386-727544505f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data privacy is crucial because it protects individuals' autonomy and control over their personal information, preventing misuse, discrimination, and potential harm.\n",
      "\n",
      "The capital of Australia is **Canberra**.\n",
      "\n",
      "Okay, imagine you have a set of Russian nesting dolls, those wooden dolls that fit inside each other.\n",
      "\n",
      "**Recursion is like opening a Russian nesting doll.**\n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "1. **You have a problem to solve (like finding the smallest doll).**\n",
      "2. **You have a main doll (the first one you see).**\n",
      "3. **To solve the problem, you open the main doll and find another doll inside.**\n",
      "4. **You ask yourself: Is *this* the smallest doll? If it is, you're done! That's your answer.**\n",
      "5. **If it's *not* the smallest doll, you repeat the process! You open this new doll and find *another* doll inside!**\n",
      "6. **You keep doing this until you finally find the smallest doll.**\n",
      "7. **Once you find the smallest doll (the *base case*), you stop! You know you've found the answer.**\n",
      "\n",
      "**Let's break it down with the key ideas:**\n",
      "\n",
      "*   **Base Case:** This is the stopping point. Like the smallest doll. If you didn't have a base case, you'd keep opening dolls forever! In computer code, this is the condition that tells the function to stop calling itself.\n",
      "\n",
      "*   **Recursive Step:** This is the part where you open the doll and find another doll inside. In computer code, this is where the function calls itself with a slightly smaller or simpler version of the problem.\n",
      "\n",
      "**Think of it like a set of instructions:**\n",
      "\n",
      "\"To find the smallest Russian doll:\n",
      "\n",
      "1.  Open the doll in front of you.\n",
      "2.  If the doll you opened is the smallest doll, stop! You found it.\n",
      "3.  If the doll you opened is *not* the smallest doll, go back to step 1 with the new doll you just found.\"\n",
      "\n",
      "**Example with Cleaning Your Room:**\n",
      "\n",
      "Let's say your mom tells you to clean your room using recursion (even though she probably wouldn't!).\n",
      "\n",
      "*   **Problem:** Clean your room.\n",
      "*   **Base Case:** If there are no toys, clothes, or anything else messy in your room, you're done! Your room is clean.\n",
      "*   **Recursive Step:**\n",
      "    1.  Find *one* messy thing in your room (a toy, a shirt, etc.).\n",
      "    2.  Put that messy thing away.\n",
      "    3.  Now, go back to the beginning of the process and repeat! (Is your room still messy? Find another thing to put away...)\n",
      "\n",
      "You keep repeating this process until your room is clean!\n",
      "\n",
      "**In Computer Programming:**\n",
      "\n",
      "In programming, recursion is a technique where a function calls *itself* within its own definition.  It's like the Russian doll analogy. The function keeps calling itself, each time working on a smaller part of the problem, until it reaches a \"base case\" where it knows the answer and can stop.\n",
      "\n",
      "**Why is recursion useful?**\n",
      "\n",
      "Sometimes, recursion can make code easier to understand and more elegant, especially when dealing with problems that can be naturally broken down into smaller, self-similar subproblems (like the Russian dolls or cleaning your room one item at a time).\n",
      "\n",
      "**Important Note:**  If you don't define a proper base case, your recursive function will keep calling itself forever (or until the computer runs out of memory). That's like having an infinite number of nesting dolls, which would be a problem!\n",
      "\n",
      "So, recursion is like opening Russian dolls, one after the other, until you find the smallest one. It's a way to solve problems by breaking them down into smaller, similar problems, and then solving those smaller problems in the same way until you reach a point where the answer is obvious.\n",
      "\n",
      "Blue waves crash and foam,\n",
      "Salty air and seagull's cry,\n",
      "Deep mystery sleeps. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Summarize the importance of data privacy in one sentence.\",\n",
    "    \"What's the capital of Australia?\",\n",
    "    \"Explain recursion to a 12-year-old.\",\n",
    "    \"Write a haiku about the ocean.\"\n",
    "]\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "gemini_outputs = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    response = model.generate_content(prompt)\n",
    "    print(response.text)\n",
    "    gemini_outputs.append(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0ad03a6-8c3d-49d8-95a9-15b7814a3fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Using cached wandb-0.19.9-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from wandb) (5.29.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Using cached sentry_sdk-2.25.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.5-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from wandb) (75.8.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from wandb) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from pydantic<3->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from pydantic<3->wandb) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n",
      "Using cached wandb-0.19.9-py3-none-win_amd64.whl (20.2 MB)\n",
      "Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Using cached sentry_sdk-2.25.1-py2.py3-none-any.whl (339 kB)\n",
      "Downloading setproctitle-1.3.5-cp312-cp312-win_amd64.whl (12 kB)\n",
      "Installing collected packages: setproctitle, sentry-sdk, docker-pycreds, wandb\n",
      "Successfully installed docker-pycreds-0.4.0 sentry-sdk-2.25.1 setproctitle-1.3.5 wandb-0.19.9\n"
     ]
    }
   ],
   "source": [
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52e69a1c-ba26-46f4-8c6c-0999f430e11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eval_run_1</strong> at: <a href='https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval/runs/le238vh6' target=\"_blank\">https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval/runs/le238vh6</a><br> View project at: <a href='https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval' target=\"_blank\">https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250408_193359-le238vh6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\riaka\\wandb\\run-20250408_193502-fvchvifk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval/runs/fvchvifk' target=\"_blank\">eval_run_1</a></strong> to <a href='https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval' target=\"_blank\">https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval/runs/fvchvifk' target=\"_blank\">https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval/runs/fvchvifk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Prompt: Summarize the importance of data privacy in one sentence.\n",
      "🤖 Gemini: Data privacy is crucial for protecting individuals' autonomy, dignity, and control over their personal information, preventing potential harms like discrimination, identity theft, and surveillance.\n",
      "🦙 Mistral: Data privacy is crucial as it safeguards individuals' personal information and upholds their rights to control how their private details are collected, stored, and used by others.\n",
      "\n",
      "🔹 Prompt: What's the capital of Australia?\n",
      "🤖 Gemini: The capital of Australia is **Canberra**.\n",
      "🦙 Mistral: The capital city of Australia is Canberra. It became the capital in 1927, replacing Melbourne and Sydney which had shared the role up until that point. Located in the Australian Capital Territory (ACT), Canberra serves as the political center of the country while each state has its own largest city as an economic and cultural hub.\n",
      "\n",
      "🔹 Prompt: Explain recursion to a 12-year-old.\n",
      "🤖 Gemini: Okay, imagine you have a set of Russian nesting dolls (Matryoshka dolls). You open the biggest one, and inside is a slightly smaller doll. You open that one, and inside is an even smaller doll. This keeps happening until you get to the tiniest doll that you can't open anymore.\n",
      "\n",
      "**That's basically recursion!**\n",
      "\n",
      "Here's the breakdown:\n",
      "\n",
      "*   **The Big Doll (The Main Problem):** You have a task you want to complete (like opening all the dolls).\n",
      "*   **The Smaller Doll (A Smaller Version of the Problem):** Instead of solving the whole big problem at once, you break it down into a smaller, similar problem (opening the next doll).\n",
      "*   **Doing the Same Thing Again and Again (Calling the Same Function):** You keep doing the same thing (opening a doll) to each smaller doll. This is like a function calling itself!\n",
      "*   **The Tiniest Doll (The Base Case):**  Eventually, you reach a point where you *know* the answer right away (the tiniest doll is already opened). This is called the \"base case\" and it stops the recursion from going on forever. Without a base case, it would be like opening dolls forever!\n",
      "\n",
      "**Let's use a real-world example like counting down from a number:**\n",
      "\n",
      "Imagine you want to write instructions for counting down from any number. You could use recursion!\n",
      "\n",
      "```\n",
      "// Pretend this is code that speaks to you\n",
      "\n",
      "function countDown(number) {\n",
      "  // Base Case: If the number is 0, we're done!  Just say \"Blastoff!\"\n",
      "  if (number === 0) {\n",
      "    console.log(\"Blastoff!\");\n",
      "  } else {\n",
      "    // Otherwise, say the number...\n",
      "    console.log(number);\n",
      "\n",
      "    // ...and then count down from the next smaller number!  (Recursion!)\n",
      "    countDown(number - 1);\n",
      "  }\n",
      "}\n",
      "\n",
      "// Let's start counting down from 3\n",
      "countDown(3);\n",
      "```\n",
      "\n",
      "**What happens when we run `countDown(3)`?**\n",
      "\n",
      "1.  `countDown(3)`: `number` is 3. It's not 0, so it prints \"3\" and then calls `countDown(2)`.\n",
      "2.  `countDown(2)`: `number` is 2. It's not 0, so it prints \"2\" and then calls `countDown(1)`.\n",
      "3.  `countDown(1)`: `number` is 1. It's not 0, so it prints \"1\" and then calls `countDown(0)`.\n",
      "4.  `countDown(0)`: `number` is 0!  This is the base case! It prints \"Blastoff!\".  The function stops.\n",
      "\n",
      "**So, the output will be:**\n",
      "\n",
      "```\n",
      "3\n",
      "2\n",
      "1\n",
      "Blastoff!\n",
      "```\n",
      "\n",
      "**Why is recursion useful?**\n",
      "\n",
      "*   **Elegant Solutions:**  For some problems, recursion can make the code easier to understand and more elegant than using loops.\n",
      "*   **Tree Structures:**  Recursion is really good for working with data that's organized like a tree (like a family tree, or folders on your computer).\n",
      "\n",
      "**Important Notes:**\n",
      "\n",
      "*   **Stack Overflow:**  If you don't have a base case, or the base case is never reached, the recursion will go on forever, and you'll get a \"stack overflow\" error (like running out of space to keep track of all the dolls you're opening).\n",
      "*   **Sometimes Slower:** Recursion can sometimes be slower than using loops because calling a function takes a little extra time.\n",
      "\n",
      "**In short:**\n",
      "\n",
      "Recursion is like solving a big problem by breaking it down into smaller, similar problems, until you get to a problem that's so easy you can solve it right away (the base case). It's like Russian nesting dolls, where each doll contains a smaller version of itself until you reach the smallest doll.\n",
      "🦙 Mistral: Sure, let's imagine you have a puzzle where every piece fits into several other pieces, and those smaller pieces also fit into even smaller ones, but eventually you find a piece that doesn't fit into anything else - it's the base case. Recursion is like solving this puzzle by only picking up each piece if it fits into another piece that has already been solved or is the base case.\n",
      "\n",
      "For example, let's say you have a set of Russian nesting dolls (matryoshka). The big one can open to reveal a smaller version inside. If that small one also opens, you find an even smaller doll, and so on. Each doll opening reveals another one, but eventually, there will be a tiny doll that doesn't have any smaller ones inside it - that's our base case.\n",
      "\n",
      "Now, solving this puzzle recursively means you start with the biggest doll. If it has a smaller one inside, you put that one aside and solve the smaller one. The rule is, you only solve each smaller doll if its bigger version has already been solved or is the base case (the smallest doll). This way, you can solve all the dolls in order, from big to small, just by following a few simple rules: if it fits inside another doll that's already been solved or is the base case, then solve it.\n",
      "\n",
      "In programming, recursion works the same way! A function calls itself, but only under certain conditions (the base case). This way, you can solve complex problems step by step, breaking them down into smaller and smaller pieces until you reach a simple case that can be easily solved without needing to break down further.\n",
      "\n",
      "🔹 Prompt: Write a haiku about the ocean.\n",
      "🤖 Gemini: Blue waves crash and roar,\n",
      "Salty air and sun-kissed skin,\n",
      "Deep secrets below.\n",
      "🦙 Mistral: Endless blue expanse,\n",
      "\n",
      "Whispers of ancient tales,\n",
      "Brine sings to the stars.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>gemini_response</td><td>Blue waves crash and...</td></tr><tr><td>mistral_response</td><td>Endless blue expanse...</td></tr><tr><td>prompt</td><td>Write a haiku about ...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eval_run_1</strong> at: <a href='https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval/runs/fvchvifk' target=\"_blank\">https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval/runs/fvchvifk</a><br> View project at: <a href='https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval' target=\"_blank\">https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250408_193502-fvchvifk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import ollama\n",
    "from google import generativeai as genai\n",
    "\n",
    "# 🔐 Keys & config\n",
    "\n",
    "\n",
    "# 🧠 Init wandb\n",
    "wandb.init(project=\"gemini-vs-mistral-eval\", name=\"eval_run_1\")\n",
    "\n",
    "# 🔁 Prompt list\n",
    "prompts = [\n",
    "    \"Summarize the importance of data privacy in one sentence.\",\n",
    "    \"What's the capital of Australia?\",\n",
    "    \"Explain recursion to a 12-year-old.\",\n",
    "    \"Write a haiku about the ocean.\"\n",
    "]\n",
    "\n",
    "# 🚀 Loop through prompts\n",
    "for prompt in prompts:\n",
    "    # Gemini Response\n",
    "    gemini_response = model.generate_content(prompt).text.strip()\n",
    "\n",
    "    # Mistral Response (from Ollama)\n",
    "    mistral_response = ollama.chat(model=\"mistral\", messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ])[\"message\"][\"content\"].strip()\n",
    "\n",
    "    # 📝 Print to console\n",
    "    print(f\"\\n🔹 Prompt: {prompt}\")\n",
    "    print(f\"🤖 Gemini: {gemini_response}\")\n",
    "    print(f\"🦙 Mistral: {mistral_response}\")\n",
    "\n",
    "    # 📊 Log to wandb\n",
    "    wandb.log({\n",
    "        \"prompt\": prompt,\n",
    "        \"gemini_response\": gemini_response,\n",
    "        \"mistral_response\": mistral_response\n",
    "    })\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d30a9e8f-11b9-4bb9-869b-cadaf4127a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.4.7-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from ollama) (0.27.0)\n",
      "Collecting pydantic<3.0.0,>=2.9.0 (from ollama)\n",
      "  Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.6.0)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic<3.0.0,>=2.9.0->ollama)\n",
      "  Downloading pydantic_core-2.33.1-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic<3.0.0,>=2.9.0->ollama)\n",
      "  Using cached typing_extensions-4.13.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.9.0->ollama)\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading ollama-0.4.7-py3-none-any.whl (13 kB)\n",
      "Downloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.1-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 13.6 MB/s eta 0:00:00\n",
      "Using cached typing_extensions-4.13.1-py3-none-any.whl (45 kB)\n",
      "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-extensions, typing-inspection, pydantic-core, pydantic, ollama\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.20.1\n",
      "    Uninstalling pydantic_core-2.20.1:\n",
      "      Successfully uninstalled pydantic_core-2.20.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.8.2\n",
      "    Uninstalling pydantic-2.8.2:\n",
      "      Successfully uninstalled pydantic-2.8.2\n",
      "Successfully installed ollama-0.4.7 pydantic-2.11.3 pydantic-core-2.33.1 typing-extensions-4.13.1 typing-inspection-0.4.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d7ae30b-6258-4304-bfda-a2df2b5b9ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "gemini_len = len(gemini_response.split())\n",
    "mistral_len = len(mistral_response.split())\n",
    "print(gemini_len)\n",
    "print(mistral_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e17861a-052c-4b02-9f9e-34b08e6ec939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\riaka\\wandb\\run-20250408_194643-6o9agtkr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/riakabra1-na/uncategorized/runs/6o9agtkr' target=\"_blank\">mild-cosmos-1</a></strong> to <a href='https://wandb.ai/riakabra1-na/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/riakabra1-na/uncategorized' target=\"_blank\">https://wandb.ai/riakabra1-na/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/riakabra1-na/uncategorized/runs/6o9agtkr' target=\"_blank\">https://wandb.ai/riakabra1-na/uncategorized/runs/6o9agtkr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init()\n",
    "wandb.log({\n",
    "    \"gemini_word_count\": gemini_len,\n",
    "    \"mistral_word_count\": mistral_len\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90617a11-3329-4d39-8abe-ab15594bd792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textstat\n",
      "  Downloading textstat-0.7.5-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pyphen (from textstat)\n",
      "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting cmudict (from textstat)\n",
      "  Downloading cmudict-1.0.32-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from textstat) (75.8.2)\n",
      "Requirement already satisfied: importlib-metadata>=5 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from cmudict->textstat) (7.0.1)\n",
      "Collecting importlib-resources>=5 (from cmudict->textstat)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\riaka\\anaconda3\\lib\\site-packages (from importlib-metadata>=5->cmudict->textstat) (3.17.0)\n",
      "Downloading textstat-0.7.5-py3-none-any.whl (105 kB)\n",
      "Downloading cmudict-1.0.32-py3-none-any.whl (939 kB)\n",
      "   ---------------------------------------- 0.0/939.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 939.4/939.4 kB 4.3 MB/s eta 0:00:00\n",
      "Downloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 14.5 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: pyphen, importlib-resources, cmudict, textstat\n",
      "Successfully installed cmudict-1.0.32 importlib-resources-6.5.2 pyphen-0.17.2 textstat-0.7.5\n"
     ]
    }
   ],
   "source": [
    "#!pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "141017e6-ee3d-4a47-b0d0-a846064e5dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "gemini_readability = textstat.flesch_reading_ease(gemini_response)\n",
    "mistral_readability = textstat.flesch_reading_ease(mistral_response)\n",
    "\n",
    "wandb.log({\n",
    "    \"gemini_readability\": gemini_readability,\n",
    "    \"mistral_readability\": mistral_readability\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b566cf9b-10f1-4a11-a57b-3eab18782383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "wandb.log({\n",
    "    \"prompt\": prompt,\n",
    "    \"gemini_response\": gemini_response,\n",
    "    \"mistral_response\": mistral_response,\n",
    "    \"gemini_word_count\": len(gemini_response.split()),\n",
    "    \"mistral_word_count\": len(mistral_response.split()),\n",
    "    \"gemini_readability\": textstat.flesch_reading_ease(gemini_response),\n",
    "    \"mistral_readability\": textstat.flesch_reading_ease(mistral_response)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "931e52c9-0ffc-494c-b43c-caed6c70f5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>gemini_readability</td><td>▁▁</td></tr><tr><td>gemini_word_count</td><td>▁▁</td></tr><tr><td>mistral_readability</td><td>▁▁</td></tr><tr><td>mistral_word_count</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>gemini_readability</td><td>92.12</td></tr><tr><td>gemini_response</td><td>Blue waves crash and...</td></tr><tr><td>gemini_word_count</td><td>13</td></tr><tr><td>mistral_readability</td><td>84.68</td></tr><tr><td>mistral_response</td><td>Endless blue expanse...</td></tr><tr><td>mistral_word_count</td><td>12</td></tr><tr><td>prompt</td><td>Write a haiku about ...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mild-cosmos-1</strong> at: <a href='https://wandb.ai/riakabra1-na/uncategorized/runs/6o9agtkr' target=\"_blank\">https://wandb.ai/riakabra1-na/uncategorized/runs/6o9agtkr</a><br> View project at: <a href='https://wandb.ai/riakabra1-na/uncategorized' target=\"_blank\">https://wandb.ai/riakabra1-na/uncategorized</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250408_194643-6o9agtkr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\riaka\\wandb\\run-20250408_200513-frd7sp5c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval/runs/frd7sp5c' target=\"_blank\">eval_run_with_metrics</a></strong> to <a href='https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval' target=\"_blank\">https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval/runs/frd7sp5c' target=\"_blank\">https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval/runs/frd7sp5c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>gemini_readability</td><td>▁▄██</td></tr><tr><td>gemini_word_count</td><td>▁▁█▁</td></tr><tr><td>mistral_readability</td><td>▁▆██</td></tr><tr><td>mistral_word_count</td><td>▁▂█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>gemini_readability</td><td>83.66</td></tr><tr><td>gemini_response</td><td>Blue waves crash and...</td></tr><tr><td>gemini_word_count</td><td>13</td></tr><tr><td>mistral_readability</td><td>67.76</td></tr><tr><td>mistral_response</td><td>Endless blue expanse...</td></tr><tr><td>mistral_word_count</td><td>12</td></tr><tr><td>prompt</td><td>Write a haiku about ...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eval_run_with_metrics</strong> at: <a href='https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval/runs/frd7sp5c' target=\"_blank\">https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval/runs/frd7sp5c</a><br> View project at: <a href='https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval' target=\"_blank\">https://wandb.ai/riakabra1-na/gemini-vs-mistral-eval</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250408_200513-frd7sp5c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import ollama\n",
    "import textstat\n",
    "from google import generativeai as genai\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "wandb.init(project=\"gemini-vs-mistral-eval\", name=\"eval_run_with_metrics\")\n",
    "\n",
    "prompts = [\n",
    "    \"Summarize the importance of data privacy in one sentence.\",\n",
    "    \"What's the capital of Australia?\",\n",
    "    \"Explain recursion to a 12-year-old.\",\n",
    "    \"Write a haiku about the ocean.\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    gemini_response = model.generate_content(prompt).text.strip()\n",
    "    mistral_response = ollama.chat(model=\"mistral\", messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ])[\"message\"][\"content\"].strip()\n",
    "\n",
    "    # Custom metrics\n",
    "    gemini_word_count = len(gemini_response.split())\n",
    "    mistral_word_count = len(mistral_response.split())\n",
    "    gemini_readability = textstat.flesch_reading_ease(gemini_response)\n",
    "    mistral_readability = textstat.flesch_reading_ease(mistral_response)\n",
    "\n",
    "    # Log everything\n",
    "    wandb.log({\n",
    "        \"prompt\": prompt,\n",
    "        \"gemini_response\": gemini_response,\n",
    "        \"mistral_response\": mistral_response,\n",
    "        \"gemini_word_count\": gemini_word_count,\n",
    "        \"mistral_word_count\": mistral_word_count,\n",
    "        \"gemini_readability\": gemini_readability,\n",
    "        \"mistral_readability\": mistral_readability\n",
    "    })\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a001e1a0-78d3-438f-9566-24b6f12c21ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
